## cs5293sp23-project3 (Text - Analytics Project 3)
## Author - Pranav Vichare
## The Smart City Slicker
**About:**  Project 3's purpose is to  to investigate themes and similarities for smart cities with the use of cluster analysis, topic modeling, and summarization. Assume you are a stakeholder in a rising Smart City and want to know more about themes and concepts about existing smart cities. You also want to know where does your smart city place among others. The following steps should be taken.
1. Download and clean pdf documents.
2. Create and explore clustering models.
3. Perform topic modeling to derive meanings.
4. Extract a summary and keywords for each smart city document.

*The **project3.ipynb** files contains the different clustering techniques used to determine the optimal k value*

To run the code, Use the commands below in terminal or Visual Studio Code
```python
pipenv --python 3.9  #use your version of python which is installed on your system. This code is also used to create a virtual environment
pipenv install scikit-learn==1.0.2 #to install scikit-learn library in virtual environment
pipenv install nltk #to install nltk library in virtual environment
pipenv install pandas #to install pandas library in virtual environment
pipenv install pytest #to install pytest library in virtual environment
pipenv install numpy #to install numpy library in virtual environment
pipenv install bs4 #to install bs4 library in virtual environment
pipenv install spacy #to install spacy library in virtual environment
pipenv install pypdf2 #to install pypdf2 library in virtual environment
#to install spacy en_core_web_sm model in virtual environment
pipenv install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl
pipenv lock #to create piplock file
```

```python
import argparse
import sys
import os
import pandas as pd
import PyPDF2
import nltk
nltk.download('stopwords', quiet = True)
import spacy
import unicodedata
import re
from nltk.corpus import wordnet
import collections
from nltk.tokenize.toktok import ToktokTokenizer
from bs4 import BeautifulSoup
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from joblib import load
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize, sent_tokenize
from collections import Counter
import numpy as np
```
To run the code, Import all the libraries listed above in the python code.

To run the code from the terminal use below command:
```
pipenv run python project3.py --document city.pdf --summarize --keywords"
```

### Code Explanation  
```python
#project3.py, normalizeText.py and predict.py has 7 functions as shown below.
def load_file()
def main()
def normalize_text()
def cluster()
def topic()
def keywords_text()
def summary()
```

When the script is executed, **the main()** method is invoked. It takes an args parameter, which is parsed by argparse. The function calls **load_file()**. The **load_file()** function loads PDF files from a given directory, extracts relevant information such as state, city, and raw text from each file, and returns a Pandas dataframe.

The **main()** function takes the path of a PDF file as input and performs several operations such as text normalization, clustering, topic modeling, summarization, and keyword extraction on the data in the dataframe generated by the **load_file()** function. The output of this function is printed in the terminal and also stored in a .tsv file.

The **CONTRACTION_MAP** dictionary maps commonly used English contractions to their extended equivalents in Python. During natural language processing activities, this vocabulary is utilized to extend contractions in text input. The code also downloads the Natural Language Toolkit (NLTK) stopwords corpus and imports additional packages such as spacy and re. It defines a set of custom stopwords as well as a ToktokTokenizer object for text tokenization. For cleaning HTML content, the BeautifulSoup package is imported.

The function The **normalize_text()** function takes a string of text as input and performs several text normalization functions on it.

**cluster()** is a function that takes a dataframe as input and returns it with an extra column specifying the cluster ID of each row. This tool predicts the cluster ID of each row based on its normalized text using a pre-trained KMeans model. 

**topic()** is a function that takes a dataframe as input and returns the same dataframe with an additional column representing each row's top two topic IDs. This function computes the topic distribution of each row's normalized text using a pre-trained LDA model, and then assigns a topic ID to each row based on the top two subjects.

**keywords_text()** is a function that takes in a text string and returns a list of the text's most essential terms. This function tokenizes the text, removes non-alphabetic tokens and stop words, stems the remaining tokens, and then selects the most frequently occurring words. 

**summary()** is a function that takes in a text string and returns a summary of it. This function tokenizes the text, removes non-alphabetic tokens and stop words, stems the remaining tokens, and then selects the most frequently occurring words. The function returns a string that is the concatenation of the selected words.

The values are stored in a **smartcity_predict.tsv** file.

*The output video of the execution with the images is stored in **docs - Execution video and images** folder*

### Code Output
The code output is stored in stored in file with name : **smartcity_predict.tsv**.
![image](https://github.com/Pranavv361/cs5293sp23-project3/blob/main/docs%20-%20Execution%20video%20and%20images/project3.py%20Execution.png)

### Test Output
The test cases are created in single file **test_project3.py**. The purpose of the test_project3.py is to check the functions with sample input and make sure we get correct output. The attached image below shows the output for test cases of all the functions.
To run the test_project3.py using pytest library use the following code.
```
pipenv run python -m pytest
```
![image](https://github.com/Pranavv361/cs5293sp23-project3/blob/main/docs%20-%20Execution%20video%20and%20images/test_project3.py%20Execution.png)

**test_normalize_text()** puts to the test the **normalize_text()** function, which accepts a string of text and returns a normalized version with no stop words or special characters.

**test_cluster()** puts to the test the **cluster()** function, which accepts a Pandas DataFrame with normalized text and returns a DataFrame with an additional column showing the cluster ID.

**test_topic()** puts to the test the **topic()** function, which accepts a Pandas DataFrame with normalized text and returns a DataFrame with an additional column showing the topic ID.

**test_keywords_text()** puts the **keywords_text()** function to the test, which accepts a string of text and returns a list of the most essential terms in the text based on frequency and relevance.

**test_summary()** puts to the test the **summary()** function, which takes a string of text and returns a summary of the most essential terms.

### Assumptions:
1. The data file should always be .pdf file.

### Bugs:   
1. Some words in keywords/summary and topics are incomplete because of normalizing techniques used.
